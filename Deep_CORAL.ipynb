{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_CORAL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yLL4y5fn4hm"
      },
      "source": [
        "pip install dalib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d-enoXln7rF"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as col\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.optim import SGD\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sys.path.append('.')\n",
        "from dalib.modules.domain_discriminator import DomainDiscriminator\n",
        "from dalib.adaptation.dann import DomainAdversarialLoss, ImageClassifier\n",
        "import dalib.vision.datasets as datasets\n",
        "import dalib.vision.models as models\n",
        "#from tools.utils import AverageMeter, ProgressMeter, accuracy, ForeverDataIterator\n",
        "#from tools.transforms import ResizeImage\n",
        "#from tools.lr_scheduler import StepwiseLR\n",
        "\n",
        "# add library\n",
        "from torchvision.models import alexnet\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# GPUが使用できるかを確認\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# RNG\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Cudnn setting\n",
        "cudnn.deterministic = False\n",
        "cudnn.benchmark = True\n",
        "\n",
        "def Office31_loader(task, batch_size,train:bool,data_dir = \"data/office31\"):\n",
        "\n",
        "    # Computed with compute_mean_std.py\n",
        "    mean_std = {\n",
        "        'A': {\n",
        "            'mean': [0.79235494, 0.7862071 , 0.78418255],\n",
        "            'std':  [0.31496558, 0.3174693 , 0.3193569 ]\n",
        "        },\n",
        "        'D': {\n",
        "            'mean': [0.47086468, 0.44865608, 0.40637794],\n",
        "            'std':  [0.20395322, 0.19204104, 0.1996422 ]\n",
        "        },\n",
        "        'W': {\n",
        "            'mean': [0.6119875 , 0.6187739 , 0.61730677],\n",
        "            'std':  [0.25063968, 0.25554898, 0.25773206]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if train == True:\n",
        "      data_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean_std[task]['mean'],\n",
        "                                 std=mean_std[task]['std'])\n",
        "        ])\n",
        "    else:\n",
        "      data_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean_std[task]['mean'],\n",
        "                                 std=mean_std[task]['std'])                                                        \n",
        "        ])\n",
        "\n",
        "\n",
        "    dataset = datasets.office31.Office31(root=data_dir, task=task, download=True, transform=data_transform)\n",
        "    dataset_loader = DataLoader(dataset, batch_size=batch_size, shuffle=train, drop_last=train)\n",
        "    return dataset,dataset_loader\n",
        "\n",
        "def coral(source, target):\n",
        "\n",
        "    d = source.size(1)  # dim vector (number of class)\n",
        "\n",
        "    source_c = compute_covariance(source)\n",
        "    target_c = compute_covariance(target)\n",
        "\n",
        "    loss = torch.sum(torch.mul((source_c - target_c), (source_c - target_c)))\n",
        "\n",
        "    loss = loss / (4 * d * d)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_covariance(input_data):\n",
        "    \"\"\"\n",
        "    Compute Covariance matrix of the input data\n",
        "    \"\"\"\n",
        "    n = input_data.size(0)  # batch_size\n",
        "\n",
        "    # Check if using gpu or cpu\n",
        "    if input_data.is_cuda:\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    id_row = torch.ones(n).resize(1, n).to(device=device)\n",
        "    sum_column = torch.mm(id_row, input_data)\n",
        "    mean_column = torch.div(sum_column, n)\n",
        "    term_mul_2 = torch.mm(mean_column.t(), mean_column)\n",
        "    d_t_d = torch.mm(input_data.t(), input_data)\n",
        "    c = torch.add(d_t_d, (-1 * term_mul_2)) * 1 / (n - 1)\n",
        "\n",
        "    return c\n",
        "\n",
        "def train(model, train_loader_dict, optimizer, lambda_coral):\n",
        "  model.train()\n",
        "  sum_cl = 0.0\n",
        "  sum_co = 0.0\n",
        "  sum_to = 0.0\n",
        "\n",
        "  train_steps = min(len(train_loader_dict['Source']), len(train_loader_dict['Target']))\n",
        "\n",
        "  for iteration in range(train_steps):\n",
        "    source_img, source_label = next(iter(train_loader_dict['Source']))\n",
        "    target_img, _ = next(iter(train_loader_dict['Target']))\n",
        "\n",
        "    source_img, source_label = source_img.to(device), source_label.to(device)\n",
        "    target_img = target_img.to(device)\n",
        "\n",
        "    source_img, source_label = Variable(source_img), Variable(source_label)\n",
        "    target_img = Variable(target_img)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    source_out = model(source_img)\n",
        "    target_out = model(target_img)\n",
        "\n",
        "    Class_loss = F.cross_entropy(source_out, source_label)\n",
        "    Coral_loss = coral(source_out, target_out)\n",
        "    Total_loss = Class_loss + Coral_loss * lambda_coral\n",
        "\n",
        "    Total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    sum_cl += Class_loss\n",
        "    sum_co += Coral_loss\n",
        "    sum_to += Total_loss\n",
        "  \n",
        "  epoch_cl, epoch_co, epoch_to = sum_cl / train_steps, sum_co / train_steps, sum_to / train_steps\n",
        "  print('class : {:.4f}, coral : {:.4f}, total : {:.4f}'.format(epoch_cl, epoch_co, epoch_to))\n",
        "\n",
        "  return epoch_cl, epoch_co\n",
        "\n",
        "\n",
        "def test(model, val_loader_dict,phase):\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for img, label in val_loader_dict[phase]:\n",
        "      img, label = img.to(device), label.to(device)\n",
        "      out = model(img)\n",
        "\n",
        "      # sum up batch loss\n",
        "      test_loss += F.cross_entropy(out, label, size_average=False).item()\n",
        "      # get the index of the max log-probability\n",
        "      _,preds = torch.max(out, 1)\n",
        "      correct += torch.sum(preds == label.data)\n",
        "      \n",
        "    test_loss /= len(val_loader_dict[phase].dataset)\n",
        "    test_acc = correct / len(val_loader_dict[phase].dataset)\n",
        "\n",
        "    print('{} loss : {:.4f} acc : {:.4f}'.format(phase, test_loss, test_acc))\n",
        "  return test_loss, test_acc\n",
        "\n",
        "\n",
        "def main(num_epochs):\n",
        "  \n",
        "  source = 'A'\n",
        "  target = 'W'\n",
        "\n",
        "  lr = 1e-3\n",
        "  decay = 5e-4\n",
        "  momentum = 0.9\n",
        "\n",
        "  lambda_coral = 0.75\n",
        "\n",
        "  batch_size = 128\n",
        "\n",
        "  # dataset & dataloader\n",
        "  source_train_dataset, source_train_loader = Office31_loader(task = source, batch_size = batch_size, train = True)\n",
        "  target_train_dataset, target_train_loader = Office31_loader(task = target, batch_size = batch_size, train = True)\n",
        "\n",
        "  source_val_dataset, source_val_loader = Office31_loader(task = source, batch_size = batch_size, train = False)\n",
        "  target_val_dataset, target_val_loader = Office31_loader(task = target, batch_size = batch_size, train = False)\n",
        "\n",
        "  train_loader_dict ={'Source':source_train_loader, 'Target':target_train_loader}\n",
        "  val_loader_dict = {'Source':source_val_loader, 'Target':target_val_loader}\n",
        "\n",
        "  n_classes = len(source_train_loader.dataset.classes)\n",
        "\n",
        "  # AlexNet\n",
        "  model = alexnet(pretrained=True)\n",
        "  model.classifier[6] = nn.Linear(4096, n_classes)\n",
        "  torch.nn.init.normal_(model.classifier[6].weight, mean=0, std=5e-3)\n",
        "  model.classifier[6].bias.data.fill_(0.01)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  # optimizer \n",
        "  optimizer = torch.optim.SGD([\n",
        "        {'params':  model.features.parameters()},\n",
        "        {'params': model.classifier[:6].parameters()},\n",
        "        # fc8 -> 7th element (index 6) in the Sequential block\n",
        "        {'params': model.classifier[6].parameters(), 'lr': 10 * lr}\n",
        "    ], lr=lr, momentum=momentum)\n",
        "  \n",
        "  # log dict\n",
        "  history = {}\n",
        "  history['train_class_loss'] = []\n",
        "  history['train_coral_loss'] = []\n",
        "  history['test_source_loss'] = []\n",
        "  history['test_source_acc'] = []\n",
        "  history['test_target_loss'] = []\n",
        "  history['test_target_acc'] = []\n",
        "\n",
        "\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print('-----------------------')\n",
        "    print('Epoch {}/{}'.format(epoch,num_epochs))\n",
        "\n",
        "    epoch_cl, epoch_co = train(model, train_loader_dict, optimizer, lambda_coral)\n",
        "    s_test_loss, s_test_acc = test(model, val_loader_dict,'Source')\n",
        "    t_test_loss, t_test_acc = test(model, val_loader_dict,'Target')\n",
        "\n",
        "    history['train_class_loss'].append(epoch_cl)\n",
        "    history['train_coral_loss'].append(epoch_co)\n",
        "    history['test_source_loss'].append(s_test_loss)\n",
        "    history['test_source_acc'].append(s_test_acc)\n",
        "    history['test_target_loss'].append(t_test_loss)\n",
        "    history['test_target_acc'].append(t_test_acc)\n",
        "\n",
        "  return history\n",
        "\n",
        "def plot_graph(values1, values2, rng, label1, label2):\n",
        "    plt.plot(range(rng), values1, label=label1)\n",
        "    plt.plot(range(rng), values2, label=label2)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVUZVgExoA32"
      },
      "source": [
        "num_epochs = 30\n",
        "history = main(num_epochs)\n",
        "plot_graph(history['test_source_acc'], history['test_target_acc'],num_epochs,'source_acc','target_acc')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}